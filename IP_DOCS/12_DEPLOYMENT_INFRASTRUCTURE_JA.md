# ç¬¬12ç« : ãƒ‡ãƒ—ãƒ­ã‚¤ & ã‚¤ãƒ³ãƒ•ãƒ©ã‚¹ãƒˆãƒ©ã‚¯ãƒãƒ£

**ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆãƒãƒ¼ã‚¸ãƒ§ãƒ³**: 1.0.0  
**æœ€çµ‚æ›´æ–°æ—¥**: 2024å¹´12æœˆ  
**åˆ†é¡**: æ©Ÿå¯† - IP ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ  
**å¯¾è±¡å¸‚å ´**: ğŸ‡¯ğŸ‡µ æ—¥æœ¬ï¼ˆãƒ¡ã‚¤ãƒ³ï¼‰ã€ğŸ‡°ğŸ‡· éŸ“å›½ã€ğŸŒ ASEAN  

---

## 12.1 ã‚¤ãƒ³ãƒ•ãƒ©ã‚¹ãƒˆãƒ©ã‚¯ãƒãƒ£ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£æ¦‚è¦

### ã‚¨ãƒ³ã‚¿ãƒ¼ãƒ—ãƒ©ã‚¤ã‚ºã‚°ãƒ¬ãƒ¼ãƒ‰ ãƒãƒ«ãƒã‚¯ãƒ©ã‚¦ãƒ‰æˆ¦ç•¥

**ã‚¤ãƒ³ãƒ•ãƒ©ã‚¹ãƒˆãƒ©ã‚¯ãƒãƒ£ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£å›³**
```mermaid
graph TB
    subgraph "ã‚¨ãƒƒã‚¸ & CDNå±¤"
        CLOUDFLARE[CloudFlare<br/>ã‚°ãƒ­ãƒ¼ãƒãƒ«CDN + DDoSä¿è­·]
        WAF[Webã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ãƒ•ã‚¡ã‚¤ã‚¢ã‚¦ã‚©ãƒ¼ãƒ«<br/>ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ãƒ«ãƒ¼ãƒ«]
        RATE_LIMIT[ãƒ¬ãƒ¼ãƒˆåˆ¶é™<br/>APIä¿è­·]
    end
    
    subgraph "ãƒ­ãƒ¼ãƒ‰ãƒãƒ©ãƒ³ã‚·ãƒ³ã‚° & ãƒ«ãƒ¼ãƒ†ã‚£ãƒ³ã‚°"
        ALB[ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ãƒ­ãƒ¼ãƒ‰ãƒãƒ©ãƒ³ã‚µãƒ¼<br/>AWS ALB + NLB]
        API_GATEWAY[APIã‚²ãƒ¼ãƒˆã‚¦ã‚§ã‚¤<br/>Kong + AWS API Gateway]
        INGRESS[Kubernetesã‚¤ãƒ³ã‚°ãƒ¬ã‚¹<br/>NGINX + Istio]
    end
    
    subgraph "ãƒ—ãƒ©ã‚¤ãƒãƒªã‚¯ãƒ©ã‚¦ãƒ‰ - AWSï¼ˆæ—¥æœ¬ï¼‰"
        AZ1[ã‚¢ãƒ™ã‚¤ãƒ©ãƒ“ãƒªãƒ†ã‚£ã‚¾ãƒ¼ãƒ³1<br/>ap-northeast-1a]
        AZ2[ã‚¢ãƒ™ã‚¤ãƒ©ãƒ“ãƒªãƒ†ã‚£ã‚¾ãƒ¼ãƒ³2<br/>ap-northeast-1c]
        AZ3[ã‚¢ãƒ™ã‚¤ãƒ©ãƒ“ãƒªãƒ†ã‚£ã‚¾ãƒ¼ãƒ³3<br/>ap-northeast-1d]
        
        subgraph "ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ãƒˆå±¤"
            EKS[Amazon EKS<br/>Kubernetesã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼]
            FARGATE[AWS Fargate<br/>ã‚µãƒ¼ãƒãƒ¼ãƒ¬ã‚¹ã‚³ãƒ³ãƒ†ãƒŠ]
            LAMBDA[AWS Lambda<br/>ã‚µãƒ¼ãƒãƒ¼ãƒ¬ã‚¹é–¢æ•°]
            EC2[EC2ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹<br/>ã‚ªãƒ¼ãƒˆã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ã‚°ãƒ«ãƒ¼ãƒ—]
        end
        
        subgraph "ãƒ‡ãƒ¼ã‚¿å±¤"
            RDS[Amazon RDS<br/>PostgreSQL Multi-AZ]
            ELASTICACHE[ElastiCache<br/>Redisã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼]
            OPENSEARCH[OpenSearch<br/>æ¤œç´¢ & ã‚¢ãƒŠãƒªãƒ†ã‚£ã‚¯ã‚¹]
            S3[Amazon S3<br/>ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸]
        end
        
        subgraph "AI/MLã‚µãƒ¼ãƒ“ã‚¹"
            SAGEMAKER[Amazon SageMaker<br/>MLå­¦ç¿’ & æ¨è«–]
            BEDROCK[Amazon Bedrock<br/>åŸºç›¤ãƒ¢ãƒ‡ãƒ«]
            TEXTRACT[Amazon Textract<br/>æ–‡æ›¸å‡¦ç†]
            COMPREHEND[Amazon Comprehend<br/>NLPã‚µãƒ¼ãƒ“ã‚¹]
        end
    end
    
    subgraph "ã‚»ã‚«ãƒ³ãƒ€ãƒªã‚¯ãƒ©ã‚¦ãƒ‰ - Azureï¼ˆéŸ“å›½ï¼‰"
        AZURE_KOREA[Azure Korea Central<br/>ç½å®³å¾©æ—§]
        AKS[Azure Kubernetes Service<br/>ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼]
        AZURE_DB[Azure Database<br/>PostgreSQL]
        AZURE_STORAGE[Azure Blob Storage<br/>ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸]
    end
    
    subgraph "ç›£è¦– & å¯è¦³æ¸¬æ€§"
        PROMETHEUS[Prometheus<br/>ãƒ¡ãƒˆãƒªã‚¯ã‚¹åé›†]
        GRAFANA[Grafana<br/>å¯è¦–åŒ–]
        JAEGER[Jaeger<br/>åˆ†æ•£ãƒˆãƒ¬ãƒ¼ã‚·ãƒ³ã‚°]
        ELK[ELKã‚¹ã‚¿ãƒƒã‚¯<br/>é›†ä¸­ãƒ­ã‚°]
        SENTRY[Sentry<br/>ã‚¨ãƒ©ãƒ¼è¿½è·¡]
    end
    
    CLOUDFLARE --> ALB
    ALB --> API_GATEWAY
    API_GATEWAY --> INGRESS
    INGRESS --> EKS
    
    EKS --> RDS
    EKS --> ELASTICACHE
    EKS --> OPENSEARCH
    EKS --> S3
    
    RDS --> AZURE_DB
    S3 --> AZURE_STORAGE
```

### ã‚¤ãƒ³ãƒ•ãƒ©ã‚¹ãƒˆãƒ©ã‚¯ãƒãƒ£æŠ€è¡“ã‚¹ã‚¿ãƒƒã‚¯

**åŒ…æ‹¬çš„ã‚¤ãƒ³ãƒ•ãƒ©ã‚¹ãƒˆãƒ©ã‚¯ãƒãƒ£ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆ**
```yaml
infrastructure_stack:
  cloud_providers:
    primary: 
      provider: "AWS"
      regions: ["ap-northeast-1", "ap-northeast-3"]
      reason: "æ—¥æœ¬å¸‚å ´ã§ã®æœ€é«˜ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã€åŒ…æ‹¬çš„ãªAI/MLã‚µãƒ¼ãƒ“ã‚¹"
      
    secondary:
      provider: "Microsoft Azure"
      regions: ["Korea Central", "Korea South"]
      reason: "éŸ“å›½ã§ã®æˆ¦ç•¥çš„ãƒ—ãƒ¬ã‚¼ãƒ³ã‚¹ã€ã‚¨ãƒ³ã‚¿ãƒ¼ãƒ—ãƒ©ã‚¤ã‚ºãƒ‘ãƒ¼ãƒˆãƒŠãƒ¼ã‚·ãƒƒãƒ—"
      
    edge:
      provider: "CloudFlare"
      global_presence: true
      reason: "å„ªã‚ŒãŸDDoSä¿è­·ã€ã‚°ãƒ­ãƒ¼ãƒãƒ«CDNãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹"

  compute_services:
    container_orchestration:
      technology: "Kubernetes"
      managed_service: "Amazon EKS"
      node_groups:
        - name: "general"
          instance_types: ["m6i.large", "m6i.xlarge", "m6i.2xlarge"]
          scaling: "ã‚ªãƒ¼ãƒˆã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ã‚°ãƒ«ãƒ¼ãƒ—"
        - name: "compute-optimized"
          instance_types: ["c6i.xlarge", "c6i.2xlarge", "c6i.4xlarge"]
          use_case: "AI/MLãƒ¯ãƒ¼ã‚¯ãƒ­ãƒ¼ãƒ‰"
        - name: "memory-optimized"
          instance_types: ["r6i.xlarge", "r6i.2xlarge", "r6i.4xlarge"]
          use_case: "ãƒ‡ãƒ¼ã‚¿å‡¦ç†ã€ã‚­ãƒ£ãƒƒã‚·ãƒ¥"
      
    serverless:
      containers: "AWS Fargate"
      functions: "AWS Lambda"
      benefits: ["ã‚³ã‚¹ãƒˆæœ€é©åŒ–", "ã‚ªãƒ¼ãƒˆã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°", "ã‚µãƒ¼ãƒãƒ¼ç®¡ç†ä¸è¦"]
      
    traditional_compute:
      ec2_instances: "ãƒªã‚¶ãƒ¼ãƒ–ãƒ‰ + ã‚ªãƒ³ãƒ‡ãƒãƒ³ãƒ‰"
      auto_scaling: "ã‚¿ãƒ¼ã‚²ãƒƒãƒˆè¿½è·¡ + äºˆæ¸¬ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°"
      
  networking:
    vpc_design:
      cidr: "10.0.0.0/16"
      subnets:
        public: ["10.0.1.0/24", "10.0.2.0/24", "10.0.3.0/24"]
        private: ["10.0.11.0/24", "10.0.12.0/24", "10.0.13.0/24"]
        database: ["10.0.21.0/24", "10.0.22.0/24", "10.0.23.0/24"]
      
    load_balancing:
      application: "AWS Application Load Balancer"
      network: "AWS Network Load Balancer"
      global: "CloudFlare Load Balancing"
      
    security:
      firewalls: "AWS Security Groups + NACLs"
      waf: "CloudFlare WAF + AWS WAF"
      vpn: "AWS VPN + Private Link"
      
  data_services:
    relational_database:
      service: "Amazon RDS for PostgreSQL"
      version: "PostgreSQL 16"
      configuration: "ãƒªãƒ¼ãƒ‰ãƒ¬ãƒ—ãƒªã‚«ä»˜ãMulti-AZ"
      backup: "ãƒã‚¤ãƒ³ãƒˆã‚¤ãƒ³ã‚¿ã‚¤ãƒ å¾©æ—§ + ã‚¯ãƒ­ã‚¹ãƒªãƒ¼ã‚¸ãƒ§ãƒ³ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—"
      
    caching:
      service: "Amazon ElastiCache for Redis"
      version: "Redis 7.x"
      configuration: "ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ãƒ¢ãƒ¼ãƒ‰æœ‰åŠ¹"
      persistence: "RDB + AOF"
      
    search_analytics:
      service: "Amazon OpenSearch Service"
      version: "OpenSearch 2.x"
      configuration: "Multi-AZã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼"
      features: ["å…¨æ–‡æ¤œç´¢", "ã‚¢ãƒŠãƒªãƒ†ã‚£ã‚¯ã‚¹", "æ©Ÿæ¢°å­¦ç¿’"]
      
    object_storage:
      service: "Amazon S3"
      storage_classes: ["Standard", "IA", "Glacier", "Deep Archive"]
      features: ["ãƒãƒ¼ã‚¸ãƒ§ãƒ‹ãƒ³ã‚°", "æš—å·åŒ–", "ã‚¯ãƒ­ã‚¹ãƒªãƒ¼ã‚¸ãƒ§ãƒ³ãƒ¬ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³"]
      
  security_compliance:
    encryption:
      at_rest: "AWS KMS + Customer Managed Keys"
      in_transit: "TLS 1.3 + mTLS"
      application: "ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ãƒ¬ãƒ™ãƒ«æš—å·åŒ–"
      
    access_control:
      identity: "AWS IAM + RBAC"
      secrets: "AWS Secrets Manager + HashiCorp Vault"
      certificates: "AWS Certificate Manager"
      
    compliance:
      frameworks: ["ISO 27001", "SOC 2 Type II", "GDPR", "JISQ 15001"]
      audit_logging: "AWS CloudTrail + ã‚«ã‚¹ã‚¿ãƒ ç›£æŸ»ã‚µãƒ¼ãƒ“ã‚¹"
      
  monitoring_observability:
    metrics:
      collection: "Prometheus + CloudWatch"
      visualization: "Grafana + CloudWatch Dashboards"
      alerting: "AlertManager + CloudWatch Alarms"
      
    logging:
      aggregation: "ELKã‚¹ã‚¿ãƒƒã‚¯ (Elasticsearch, Logstash, Kibana)"
      shipping: "Fluentd + CloudWatch Logs"
      retention: "30æ—¥ãƒ›ãƒƒãƒˆã€1å¹´ã‚¦ã‚©ãƒ¼ãƒ ã€7å¹´ã‚³ãƒ¼ãƒ«ãƒ‰"
      
    tracing:
      distributed: "Jaeger + AWS X-Ray"
      application: "OpenTelemetry"
      
    error_tracking:
      service: "Sentry"
      integration: "å…¨ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã¨ã‚µãƒ¼ãƒ“ã‚¹"
```

## 12.2 ã‚³ãƒ³ãƒ†ãƒŠåŒ– & Kubernetes

### Dockerã‚³ãƒ³ãƒ†ãƒŠæˆ¦ç•¥

**æœ€é©åŒ–ã®ãŸã‚ã®ãƒãƒ«ãƒã‚¹ãƒ†ãƒ¼ã‚¸Dockerãƒ“ãƒ«ãƒ‰**
```dockerfile
# Node.jsãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰ã‚µãƒ¼ãƒ“ã‚¹ç”¨ãƒãƒ«ãƒã‚¹ãƒ†ãƒ¼ã‚¸Dockerfile
FROM node:20-alpine AS base
WORKDIR /app
COPY package*.json ./
RUN npm ci --only=production && npm cache clean --force

# é–‹ç™ºã‚¹ãƒ†ãƒ¼ã‚¸
FROM base AS development
RUN npm ci
COPY . .
EXPOSE 3000
CMD ["npm", "run", "dev"]

# ãƒ“ãƒ«ãƒ‰ã‚¹ãƒ†ãƒ¼ã‚¸
FROM base AS build
RUN npm ci
COPY . .
RUN npm run build
RUN npm prune --production

# æœ¬ç•ªã‚¹ãƒ†ãƒ¼ã‚¸
FROM node:20-alpine AS production
RUN addgroup -g 1001 -S nodejs && \
    adduser -S nextjs -u 1001
    
WORKDIR /app
COPY --from=build --chown=nextjs:nodejs /app/dist ./dist
COPY --from=build --chown=nextjs:nodejs /app/node_modules ./node_modules
COPY --from=build --chown=nextjs:nodejs /app/package.json ./package.json

USER nextjs
EXPOSE 3000
ENV NODE_ENV=production
HEALTHCHECK --interval=30s --timeout=3s --start-period=5s --retries=3 \
  CMD node healthcheck.js

CMD ["node", "dist/server.js"]

# Python AIã‚µãƒ¼ãƒ“ã‚¹ Dockerfile
FROM python:3.12-slim AS python-base
ENV PYTHONUNBUFFERED=1 \
    PYTHONDONTWRITEBYTECODE=1 \
    PIP_NO_CACHE_DIR=1 \
    PIP_DISABLE_PIP_VERSION_CHECK=1

WORKDIR /app

# ã‚·ã‚¹ãƒ†ãƒ ä¾å­˜é–¢ä¿‚ã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
RUN apt-get update && apt-get install -y \
    build-essential \
    curl \
    software-properties-common \
    && rm -rf /var/lib/apt/lists/*

# Pythoné–‹ç™ºã‚¹ãƒ†ãƒ¼ã‚¸
FROM python-base AS python-dev
COPY requirements.txt .
RUN pip install -r requirements.txt
COPY . .
CMD ["uvicorn", "main:app", "--host", "0.0.0.0", "--port", "8000", "--reload"]

# Pythonæœ¬ç•ªã‚¹ãƒ†ãƒ¼ã‚¸
FROM python-base AS python-prod
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# érootãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚’ä½œæˆ
RUN groupadd -r appuser && useradd -r -g appuser appuser
COPY --chown=appuser:appuser . .
USER appuser

EXPOSE 8000
HEALTHCHECK --interval=30s --timeout=10s --start-period=5s --retries=3 \
  CMD curl -f http://localhost:8000/health || exit 1

CMD ["gunicorn", "main:app", "-w", "4", "-k", "uvicorn.workers.UvicornWorker", "-b", "0.0.0.0:8000"]

# Next.jsãƒ•ãƒ­ãƒ³ãƒˆã‚¨ãƒ³ãƒ‰ Dockerfile
FROM node:20-alpine AS nextjs-base
RUN apk add --no-cache libc6-compat
WORKDIR /app

# ä¾å­˜é–¢ä¿‚ã‚¹ãƒ†ãƒ¼ã‚¸
FROM nextjs-base AS nextjs-deps
COPY package.json package-lock.json ./
RUN npm ci

# ãƒ“ãƒ«ãƒ€ãƒ¼ã‚¹ãƒ†ãƒ¼ã‚¸
FROM nextjs-base AS nextjs-builder
COPY --from=nextjs-deps /app/node_modules ./node_modules
COPY . .
ENV NEXT_TELEMETRY_DISABLED 1
RUN npm run build

# ãƒ©ãƒ³ãƒŠãƒ¼ã‚¹ãƒ†ãƒ¼ã‚¸
FROM nextjs-base AS nextjs-runner
RUN addgroup --system --gid 1001 nodejs
RUN adduser --system --uid 1001 nextjs

COPY --from=nextjs-builder /app/public ./public
COPY --from=nextjs-builder --chown=nextjs:nodejs /app/.next/standalone ./
COPY --from=nextjs-builder --chown=nextjs:nodejs /app/.next/static ./.next/static

USER nextjs
EXPOSE 3000
ENV PORT 3000
ENV NODE_ENV production

CMD ["node", "server.js"]
```

### Kubernetesãƒ‡ãƒ—ãƒ­ã‚¤ãƒ¡ãƒ³ãƒˆãƒãƒ‹ãƒ•ã‚§ã‚¹ãƒˆ

**å®Œå…¨ãªKubernetesæ§‹æˆ**
```yaml
# åå‰ç©ºé–“è¨­å®š
apiVersion: v1
kind: Namespace
metadata:
  name: iworkz-production
  labels:
    name: iworkz-production
    environment: production
    
---
# ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰APIãƒ‡ãƒ—ãƒ­ã‚¤ãƒ¡ãƒ³ãƒˆ
apiVersion: apps/v1
kind: Deployment
metadata:
  name: backend-api
  namespace: iworkz-production
  labels:
    app: backend-api
    tier: backend
spec:
  replicas: 3
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxSurge: 1
      maxUnavailable: 0
  selector:
    matchLabels:
      app: backend-api
  template:
    metadata:
      labels:
        app: backend-api
        tier: backend
    spec:
      serviceAccountName: backend-api-sa
      securityContext:
        runAsNonRoot: true
        runAsUser: 1001
        fsGroup: 1001
      containers:
      - name: backend-api
        image: iworkz/backend-api:latest
        imagePullPolicy: Always
        ports:
        - containerPort: 3000
          name: http
        env:
        - name: NODE_ENV
          value: "production"
        - name: DATABASE_URL
          valueFrom:
            secretKeyRef:
              name: database-secret
              key: url
        - name: REDIS_URL
          valueFrom:
            secretKeyRef:
              name: redis-secret
              key: url
        - name: JWT_SECRET
          valueFrom:
            secretKeyRef:
              name: jwt-secret
              key: secret
        resources:
          requests:
            memory: "512Mi"
            cpu: "250m"
          limits:
            memory: "1Gi"
            cpu: "500m"
        livenessProbe:
          httpGet:
            path: /health
            port: 3000
          initialDelaySeconds: 30
          periodSeconds: 10
          timeoutSeconds: 5
          failureThreshold: 3
        readinessProbe:
          httpGet:
            path: /ready
            port: 3000
          initialDelaySeconds: 5
          periodSeconds: 5
          timeoutSeconds: 3
          failureThreshold: 3
        volumeMounts:
        - name: config-volume
          mountPath: /app/config
          readOnly: true
        - name: temp-storage
          mountPath: /tmp
      volumes:
      - name: config-volume
        configMap:
          name: backend-api-config
      - name: temp-storage
        emptyDir: {}
      imagePullSecrets:
      - name: docker-registry-secret

---
# ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰APIã‚µãƒ¼ãƒ“ã‚¹
apiVersion: v1
kind: Service
metadata:
  name: backend-api-service
  namespace: iworkz-production
  labels:
    app: backend-api
spec:
  type: ClusterIP
  ports:
  - port: 80
    targetPort: 3000
    protocol: TCP
    name: http
  selector:
    app: backend-api

---
# AIã‚µãƒ¼ãƒ“ã‚¹ãƒ‡ãƒ—ãƒ­ã‚¤ãƒ¡ãƒ³ãƒˆ
apiVersion: apps/v1
kind: Deployment
metadata:
  name: ai-services
  namespace: iworkz-production
  labels:
    app: ai-services
    tier: ai
spec:
  replicas: 2
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxSurge: 1
      maxUnavailable: 0
  selector:
    matchLabels:
      app: ai-services
  template:
    metadata:
      labels:
        app: ai-services
        tier: ai
    spec:
      nodeSelector:
        instance-type: compute-optimized
      tolerations:
      - key: "ai-workload"
        operator: "Equal"
        value: "true"
        effect: "NoSchedule"
      containers:
      - name: ai-services
        image: iworkz/ai-services:latest
        imagePullPolicy: Always
        ports:
        - containerPort: 8000
          name: http
        env:
        - name: OPENAI_API_KEY
          valueFrom:
            secretKeyRef:
              name: ai-secrets
              key: openai-key
        - name: AWS_REGION
          value: "ap-northeast-1"
        - name: SAGEMAKER_ENDPOINT
          valueFrom:
            configMapKeyRef:
              name: ai-config
              key: sagemaker-endpoint
        resources:
          requests:
            memory: "2Gi"
            cpu: "1000m"
            nvidia.com/gpu: 0
          limits:
            memory: "4Gi"
            cpu: "2000m"
            nvidia.com/gpu: 1
        livenessProbe:
          httpGet:
            path: /health
            port: 8000
          initialDelaySeconds: 60
          periodSeconds: 30
          timeoutSeconds: 10
          failureThreshold: 3
        readinessProbe:
          httpGet:
            path: /ready
            port: 8000
          initialDelaySeconds: 10
          periodSeconds: 10
          timeoutSeconds: 5
          failureThreshold: 3

---
# ãƒ•ãƒ­ãƒ³ãƒˆã‚¨ãƒ³ãƒ‰Webã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ãƒ‡ãƒ—ãƒ­ã‚¤ãƒ¡ãƒ³ãƒˆ
apiVersion: apps/v1
kind: Deployment
metadata:
  name: web-frontend
  namespace: iworkz-production
  labels:
    app: web-frontend
    tier: frontend
spec:
  replicas: 4
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxSurge: 2
      maxUnavailable: 1
  selector:
    matchLabels:
      app: web-frontend
  template:
    metadata:
      labels:
        app: web-frontend
        tier: frontend
    spec:
      containers:
      - name: web-frontend
        image: iworkz/web-frontend:latest
        imagePullPolicy: Always
        ports:
        - containerPort: 3000
          name: http
        env:
        - name: NEXT_PUBLIC_API_URL
          valueFrom:
            configMapKeyRef:
              name: frontend-config
              key: api-url
        - name: NEXT_PUBLIC_GA_ID
          valueFrom:
            configMapKeyRef:
              name: frontend-config
              key: ga-id
        resources:
          requests:
            memory: "256Mi"
            cpu: "100m"
          limits:
            memory: "512Mi"
            cpu: "250m"
        livenessProbe:
          httpGet:
            path: /api/health
            port: 3000
          initialDelaySeconds: 30
          periodSeconds: 10
        readinessProbe:
          httpGet:
            path: /api/ready
            port: 3000
          initialDelaySeconds: 5
          periodSeconds: 5

---
# ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰APIç”¨æ°´å¹³ãƒãƒƒãƒ‰ã‚ªãƒ¼ãƒˆã‚¹ã‚±ãƒ¼ãƒ©ãƒ¼
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: backend-api-hpa
  namespace: iworkz-production
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: backend-api
  minReplicas: 3
  maxReplicas: 20
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 70
  - type: Resource
    resource:
      name: memory
      target:
        type: Utilization
        averageUtilization: 80
  behavior:
    scaleDown:
      stabilizationWindowSeconds: 300
      policies:
      - type: Percent
        value: 10
        periodSeconds: 60
    scaleUp:
      stabilizationWindowSeconds: 60
      policies:
      - type: Percent
        value: 50
        periodSeconds: 60

---
# SSLä»˜ãã‚¤ãƒ³ã‚°ãƒ¬ã‚¹æ§‹æˆ
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: iworkz-ingress
  namespace: iworkz-production
  annotations:
    kubernetes.io/ingress.class: nginx
    nginx.ingress.kubernetes.io/ssl-redirect: "true"
    nginx.ingress.kubernetes.io/use-regex: "true"
    nginx.ingress.kubernetes.io/rate-limit: "100"
    nginx.ingress.kubernetes.io/rate-limit-window: "1m"
    cert-manager.io/cluster-issuer: "letsencrypt-prod"
    nginx.ingress.kubernetes.io/cors-allow-origin: "https://iworkz.com"
    nginx.ingress.kubernetes.io/cors-allow-methods: "GET, POST, PUT, DELETE, OPTIONS"
    nginx.ingress.kubernetes.io/cors-allow-headers: "DNT,User-Agent,X-Requested-With,If-Modified-Since,Cache-Control,Content-Type,Authorization"
spec:
  tls:
  - hosts:
    - api.iworkz.com
    - app.iworkz.com
    secretName: iworkz-tls
  rules:
  - host: api.iworkz.com
    http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: backend-api-service
            port:
              number: 80
  - host: app.iworkz.com
    http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: web-frontend-service
            port:
              number: 80

---
# ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³è¨­å®šç”¨ConfigMap
apiVersion: v1
kind: ConfigMap
metadata:
  name: backend-api-config
  namespace: iworkz-production
data:
  app.yaml: |
    server:
      port: 3000
      host: "0.0.0.0"
      cors:
        enabled: true
        origins: ["https://app.iworkz.com"]
    
    database:
      pool:
        min: 5
        max: 20
        idle_timeout: 10000
      ssl: true
      
    redis:
      cluster_mode: true
      connection_timeout: 5000
      command_timeout: 3000
      
    logging:
      level: "info"
      format: "json"
      
    monitoring:
      prometheus:
        enabled: true
        port: 9090
      
    features:
      ai_matching: true
      voice_interface: true
      business_card_scanner: true
      
---
# ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ç”¨ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ãƒãƒªã‚·ãƒ¼
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: backend-api-netpol
  namespace: iworkz-production
spec:
  podSelector:
    matchLabels:
      app: backend-api
  policyTypes:
  - Ingress
  - Egress
  ingress:
  - from:
    - namespaceSelector:
        matchLabels:
          name: ingress-nginx
    - podSelector:
        matchLabels:
          app: web-frontend
    ports:
    - protocol: TCP
      port: 3000
  egress:
  - to: []
    ports:
    - protocol: TCP
      port: 5432  # PostgreSQL
    - protocol: TCP
      port: 6379  # Redis
    - protocol: TCP
      port: 443   # HTTPS outbound
    - protocol: TCP
      port: 53    # DNS
    - protocol: UDP
      port: 53    # DNS
```

## 12.3 CI/CDãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å®Ÿè£…

### GitLab CI/CDãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³

**åŒ…æ‹¬çš„DevOpsãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³æ§‹æˆ**
```yaml
# .gitlab-ci.yml
stages:
  - validate
  - test
  - security
  - build
  - deploy-staging
  - integration-tests
  - deploy-production
  - post-deploy

variables:
  DOCKER_REGISTRY: "registry.gitlab.com/iworkz"
  KUBERNETES_NAMESPACE_STAGING: "iworkz-staging"
  KUBERNETES_NAMESPACE_PRODUCTION: "iworkz-production"
  AWS_DEFAULT_REGION: "ap-northeast-1"

# å…±é€šDockeræ“ä½œç”¨ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ
.docker_template: &docker_template
  image: docker:24.0.5
  services:
    - docker:24.0.5-dind
  before_script:
    - docker login -u $CI_REGISTRY_USER -p $CI_REGISTRY_PASSWORD $CI_REGISTRY
    - apk add --no-cache curl git

# kubectlæ“ä½œç”¨ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ
.kubectl_template: &kubectl_template
  image: bitnami/kubectl:latest
  before_script:
    - echo $KUBE_CONFIG | base64 -d > $HOME/.kube/config
    - kubectl version --client

# æ¤œè¨¼ã‚¹ãƒ†ãƒ¼ã‚¸
validate:code-quality:
  stage: validate
  image: node:20-alpine
  script:
    - npm ci
    - npm run lint
    - npm run type-check
    - npm run format-check
  only:
    - merge_requests
    - main
    - develop

validate:dependencies:
  stage: validate
  image: node:20-alpine
  script:
    - npm ci
    - npm audit --audit-level high
    - npm run license-check
  only:
    - merge_requests
    - main

# ãƒ†ã‚¹ãƒˆã‚¹ãƒ†ãƒ¼ã‚¸
test:unit:
  stage: test
  image: node:20-alpine
  services:
    - postgres:16-alpine
    - redis:7-alpine
  variables:
    POSTGRES_DB: iworkz_test
    POSTGRES_USER: test
    POSTGRES_PASSWORD: test
    DATABASE_URL: "postgresql://test:test@postgres:5432/iworkz_test"
    REDIS_URL: "redis://redis:6379"
  script:
    - npm ci
    - npm run test:unit
    - npm run test:integration
  coverage: '/Coverage: \d+\.\d+%/'
  artifacts:
    reports:
      coverage_report:
        coverage_format: cobertura
        path: coverage/cobertura-coverage.xml
      junit: junit.xml
    paths:
      - coverage/
    expire_in: 1 week

test:e2e:
  stage: test
  image: mcr.microsoft.com/playwright:v1.40.0-focal
  services:
    - postgres:16-alpine
    - redis:7-alpine
  variables:
    DATABASE_URL: "postgresql://test:test@postgres:5432/iworkz_test"
    REDIS_URL: "redis://redis:6379"
  script:
    - npm ci
    - npm run build
    - npm run start:test &
    - sleep 30
    - npm run test:e2e
  artifacts:
    paths:
      - playwright-report/
      - test-results/
    expire_in: 1 week
    when: always

# ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ã‚¹ãƒ†ãƒ¼ã‚¸
security:container-scan:
  stage: security
  <<: *docker_template
  script:
    - docker build -t $CI_REGISTRY_IMAGE:$CI_COMMIT_SHA .
    - docker run --rm -v /var/run/docker.sock:/var/run/docker.sock 
      -v $PWD:/tmp/.cache/ aquasec/trivy:latest image 
      --exit-code 1 --severity HIGH,CRITICAL 
      $CI_REGISTRY_IMAGE:$CI_COMMIT_SHA
  only:
    - main
    - develop

security:sast:
  stage: security
  image: securecodewarrior/gitlab-sast:latest
  script:
    - /analyzer run
  artifacts:
    reports:
      sast: gl-sast-report.json
  only:
    - merge_requests
    - main

security:dependency-scan:
  stage: security
  image: node:20-alpine
  script:
    - npm ci
    - npm audit --json > dependency-scan-report.json
  artifacts:
    reports:
      dependency_scanning: dependency-scan-report.json
  only:
    - merge_requests
    - main

# ãƒ“ãƒ«ãƒ‰ã‚¹ãƒ†ãƒ¼ã‚¸
build:backend:
  stage: build
  <<: *docker_template
  script:
    - cd backend-api
    - docker build 
      --target production 
      --build-arg NODE_ENV=production 
      -t $CI_REGISTRY_IMAGE/backend-api:$CI_COMMIT_SHA 
      -t $CI_REGISTRY_IMAGE/backend-api:latest .
    - docker push $CI_REGISTRY_IMAGE/backend-api:$CI_COMMIT_SHA
    - docker push $CI_REGISTRY_IMAGE/backend-api:latest
  only:
    - main
    - develop

build:frontend:
  stage: build
  <<: *docker_template
  script:
    - cd web-frontend
    - docker build 
      --target nextjs-runner 
      --build-arg NEXT_PUBLIC_API_URL=$NEXT_PUBLIC_API_URL
      -t $CI_REGISTRY_IMAGE/web-frontend:$CI_COMMIT_SHA 
      -t $CI_REGISTRY_IMAGE/web-frontend:latest .
    - docker push $CI_REGISTRY_IMAGE/web-frontend:$CI_COMMIT_SHA
    - docker push $CI_REGISTRY_IMAGE/web-frontend:latest
  only:
    - main
    - develop

build:ai-services:
  stage: build
  <<: *docker_template
  script:
    - cd ai-services
    - docker build 
      --target python-prod 
      -t $CI_REGISTRY_IMAGE/ai-services:$CI_COMMIT_SHA 
      -t $CI_REGISTRY_IMAGE/ai-services:latest .
    - docker push $CI_REGISTRY_IMAGE/ai-services:$CI_COMMIT_SHA
    - docker push $CI_REGISTRY_IMAGE/ai-services:latest
  only:
    - main
    - develop

# ã‚¹ãƒ†ãƒ¼ã‚¸ãƒ³ã‚°ãƒ‡ãƒ—ãƒ­ã‚¤
deploy:staging:
  stage: deploy-staging
  <<: *kubectl_template
  environment:
    name: staging
    url: https://staging.iworkz.com
  script:
    - kubectl config set-context --current --namespace=$KUBERNETES_NAMESPACE_STAGING
    - envsubst < k8s/backend-api.yaml | kubectl apply -f -
    - envsubst < k8s/web-frontend.yaml | kubectl apply -f -
    - envsubst < k8s/ai-services.yaml | kubectl apply -f -
    - kubectl rollout status deployment/backend-api
    - kubectl rollout status deployment/web-frontend
    - kubectl rollout status deployment/ai-services
  variables:
    IMAGE_TAG: $CI_COMMIT_SHA
    ENVIRONMENT: staging
  only:
    - develop

# çµ±åˆãƒ†ã‚¹ãƒˆ
integration-tests:staging:
  stage: integration-tests
  image: postman/newman:latest
  script:
    - newman run tests/integration/api-tests.postman_collection.json 
      --environment tests/integration/staging.postman_environment.json
      --reporters cli,junit --reporter-junit-export newman-report.xml
  artifacts:
    reports:
      junit: newman-report.xml
  only:
    - develop
  needs:
    - deploy:staging

# æœ¬ç•ªãƒ‡ãƒ—ãƒ­ã‚¤
deploy:production:
  stage: deploy-production
  <<: *kubectl_template
  environment:
    name: production
    url: https://app.iworkz.com
  script:
    - kubectl config set-context --current --namespace=$KUBERNETES_NAMESPACE_PRODUCTION
    - |
      # Blue-Green ãƒ‡ãƒ—ãƒ­ã‚¤æˆ¦ç•¥
      CURRENT_COLOR=$(kubectl get service backend-api-service -o jsonpath='{.spec.selector.color}')
      if [ "$CURRENT_COLOR" = "blue" ]; then
        NEW_COLOR="green"
      else
        NEW_COLOR="blue"
      fi
      
      echo "Deploying to $NEW_COLOR environment"
      
      # æ–°ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã‚’ãƒ‡ãƒ—ãƒ­ã‚¤
      sed "s/COLOR_PLACEHOLDER/$NEW_COLOR/g" k8s/backend-api.yaml | envsubst | kubectl apply -f -
      sed "s/COLOR_PLACEHOLDER/$NEW_COLOR/g" k8s/web-frontend.yaml | envsubst | kubectl apply -f -
      sed "s/COLOR_PLACEHOLDER/$NEW_COLOR/g" k8s/ai-services.yaml | envsubst | kubectl apply -f -
      
      # ãƒ­ãƒ¼ãƒ«ã‚¢ã‚¦ãƒˆã®å®Œäº†ã‚’å¾…æ©Ÿ
      kubectl rollout status deployment/backend-api-$NEW_COLOR
      kubectl rollout status deployment/web-frontend-$NEW_COLOR
      kubectl rollout status deployment/ai-services-$NEW_COLOR
      
      # ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯
      sleep 30
      kubectl get pods -l color=$NEW_COLOR
      
      # ãƒˆãƒ©ãƒ•ã‚£ãƒƒã‚¯ã‚’åˆ‡ã‚Šæ›¿ãˆ
      kubectl patch service backend-api-service -p '{"spec":{"selector":{"color":"'$NEW_COLOR'"}}}'
      kubectl patch service web-frontend-service -p '{"spec":{"selector":{"color":"'$NEW_COLOR'"}}}'
      kubectl patch service ai-services-service -p '{"spec":{"selector":{"color":"'$NEW_COLOR'"}}}'
      
      echo "Traffic switched to $NEW_COLOR"
      
      # 5åˆ†å¾Œã«å¤ã„ãƒ‡ãƒ—ãƒ­ã‚¤ã‚’ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
      sleep 300
      kubectl delete deployment backend-api-$CURRENT_COLOR --ignore-not-found=true
      kubectl delete deployment web-frontend-$CURRENT_COLOR --ignore-not-found=true
      kubectl delete deployment ai-services-$CURRENT_COLOR --ignore-not-found=true
  variables:
    IMAGE_TAG: $CI_COMMIT_SHA
    ENVIRONMENT: production
  when: manual
  only:
    - main

# ãƒ‡ãƒ—ãƒ­ã‚¤å¾Œã®ã‚¿ã‚¹ã‚¯
post-deploy:monitoring:
  stage: post-deploy
  image: curlimages/curl:latest
  script:
    - |
      # ã‚µãƒ¼ãƒ“ã‚¹ã®æ­£å¸¸æ€§ã‚’ç¢ºèª
      echo "Checking service health..."
      curl -f https://api.iworkz.com/health
      curl -f https://app.iworkz.com/api/health
      
      # åˆæˆç›£è¦–ã‚’ãƒˆãƒªã‚¬ãƒ¼
      curl -X POST "$DATADOG_WEBHOOK" \
        -H "Content-Type: application/json" \
        -d '{"deployment":"'$CI_COMMIT_SHA'","environment":"production"}'
      
      # ãƒ‡ãƒ—ãƒ­ã‚¤è¿½è·¡ã‚’æ›´æ–°
      curl -X POST "$DEPLOYMENT_WEBHOOK" \
        -H "Content-Type: application/json" \
        -d '{"version":"'$CI_COMMIT_SHA'","environment":"production","status":"deployed"}'
  only:
    - main
  needs:
    - deploy:production

post-deploy:database-migration:
  stage: post-deploy
  image: migrate/migrate:latest
  script:
    - migrate -path ./migrations -database $DATABASE_URL up
  only:
    - main
  needs:
    - deploy:production
  when: manual
```

## 12.4 Infrastructure as Code (IaC)

### Terraform AWS ã‚¤ãƒ³ãƒ•ãƒ©ã‚¹ãƒˆãƒ©ã‚¯ãƒãƒ£

**å®Œå…¨ãªAWSã‚¤ãƒ³ãƒ•ãƒ©ã‚¹ãƒˆãƒ©ã‚¯ãƒãƒ£å®šç¾©**
```hcl
# terraform/main.tf
terraform {
  required_version = ">= 1.0"
  required_providers {
    aws = {
      source  = "hashicorp/aws"
      version = "~> 5.0"
    }
    kubernetes = {
      source  = "hashicorp/kubernetes"
      version = "~> 2.24"
    }
    helm = {
      source  = "hashicorp/helm"
      version = "~> 2.12"
    }
  }
  
  backend "s3" {
    bucket         = "iworkz-terraform-state"
    key            = "production/terraform.tfstate"
    region         = "ap-northeast-1"
    encrypt        = true
    dynamodb_table = "terraform-lock"
  }
}

provider "aws" {
  region = var.aws_region
  
  default_tags {
    tags = {
      Project     = "iWORKZ"
      Environment = var.environment
      ManagedBy   = "Terraform"
      Owner       = "DevOps"
    }
  }
}

# ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹
data "aws_availability_zones" "available" {
  state = "available"
}

data "aws_caller_identity" "current" {}

# å¤‰æ•°
variable "aws_region" {
  description = "AWSãƒªã‚½ãƒ¼ã‚¹ã®ãƒªãƒ¼ã‚¸ãƒ§ãƒ³"
  type        = string
  default     = "ap-northeast-1"
}

variable "environment" {
  description = "ç’°å¢ƒå"
  type        = string
  default     = "production"
}

variable "project_name" {
  description = "ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆå"
  type        = string
  default     = "iworkz"
}

variable "vpc_cidr" {
  description = "VPCã®CIDRãƒ–ãƒ­ãƒƒã‚¯"
  type        = string
  default     = "10.0.0.0/16"
}

# VPCè¨­å®š
module "vpc" {
  source = "terraform-aws-modules/vpc/aws"
  version = "~> 5.0"

  name = "${var.project_name}-${var.environment}-vpc"
  cidr = var.vpc_cidr

  azs             = slice(data.aws_availability_zones.available.names, 0, 3)
  private_subnets = ["10.0.1.0/24", "10.0.2.0/24", "10.0.3.0/24"]
  public_subnets  = ["10.0.101.0/24", "10.0.102.0/24", "10.0.103.0/24"]
  database_subnets = ["10.0.201.0/24", "10.0.202.0/24", "10.0.203.0/24"]

  enable_nat_gateway     = true
  single_nat_gateway     = false
  enable_vpn_gateway     = false
  enable_dns_hostnames   = true
  enable_dns_support     = true

  # VPCãƒ•ãƒ­ãƒ¼ãƒ­ã‚°
  enable_flow_log                      = true
  create_flow_log_cloudwatch_log_group = true
  create_flow_log_cloudwatch_iam_role  = true

  tags = {
    "kubernetes.io/cluster/${var.project_name}-${var.environment}" = "shared"
  }

  public_subnet_tags = {
    "kubernetes.io/cluster/${var.project_name}-${var.environment}" = "shared"
    "kubernetes.io/role/elb"                                       = 1
  }

  private_subnet_tags = {
    "kubernetes.io/cluster/${var.project_name}-${var.environment}" = "shared"
    "kubernetes.io/role/internal-elb"                             = 1
  }
}

# EKSã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼
module "eks" {
  source = "terraform-aws-modules/eks/aws"
  version = "~> 19.0"

  cluster_name    = "${var.project_name}-${var.environment}"
  cluster_version = "1.28"

  vpc_id                         = module.vpc.vpc_id
  subnet_ids                     = module.vpc.private_subnets
  cluster_endpoint_public_access = true
  cluster_endpoint_private_access = true

  cluster_addons = {
    coredns = {
      most_recent = true
    }
    kube-proxy = {
      most_recent = true
    }
    vpc-cni = {
      most_recent = true
    }
    aws-ebs-csi-driver = {
      most_recent = true
    }
  }

  # EKSç®¡ç†ãƒãƒ¼ãƒ‰ã‚°ãƒ«ãƒ¼ãƒ—
  eks_managed_node_groups = {
    general = {
      name = "general"
      
      instance_types = ["m6i.large", "m6i.xlarge"]
      
      min_size     = 3
      max_size     = 20
      desired_size = 6

      pre_bootstrap_user_data = <<-EOT
        #!/bin/bash
        /etc/eks/bootstrap.sh ${var.project_name}-${var.environment}
      EOT

      vpc_security_group_ids = [aws_security_group.node_group_sg.id]
      
      tags = {
        NodeGroup = "general"
      }
    }

    compute_optimized = {
      name = "compute-optimized"
      
      instance_types = ["c6i.xlarge", "c6i.2xlarge"]
      
      min_size     = 0
      max_size     = 10
      desired_size = 2

      taints = [
        {
          key    = "compute-optimized"
          value  = "true"
          effect = "NO_SCHEDULE"
        }
      ]

      labels = {
        NodeGroup = "compute-optimized"
        Workload  = "ai-ml"
      }

      tags = {
        NodeGroup = "compute-optimized"
      }
    }

    memory_optimized = {
      name = "memory-optimized"
      
      instance_types = ["r6i.xlarge", "r6i.2xlarge"]
      
      min_size     = 0
      max_size     = 5
      desired_size = 1

      taints = [
        {
          key    = "memory-optimized"
          value  = "true"
          effect = "NO_SCHEDULE"
        }
      ]

      labels = {
        NodeGroup = "memory-optimized"
        Workload  = "data-processing"
      }

      tags = {
        NodeGroup = "memory-optimized"
      }
    }
  }

  # Fargateãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«
  fargate_profiles = {
    default = {
      name = "default"
      selectors = [
        {
          namespace = "fargate-workloads"
        }
      ]

      tags = {
        Owner = "fargate"
      }

      timeouts = {
        create = "20m"
        delete = "20m"
      }
    }
  }

  # aws-auth configmap
  manage_aws_auth_configmap = true

  aws_auth_roles = [
    {
      rolearn  = module.eks_admins_iam_role.iam_role_arn
      username = "cluster-admin"
      groups   = ["system:masters"]
    },
  ]

  aws_auth_users = [
    {
      userarn  = "arn:aws:iam::${data.aws_caller_identity.current.account_id}:user/devops"
      username = "devops"
      groups   = ["system:masters"]
    },
  ]

  tags = {
    Environment = var.environment
    Terraform   = "true"
  }
}

# EKSãƒãƒ¼ãƒ‰ã‚°ãƒ«ãƒ¼ãƒ—ç”¨ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ã‚°ãƒ«ãƒ¼ãƒ—
resource "aws_security_group" "node_group_sg" {
  name_prefix = "${var.project_name}-${var.environment}-node-group"
  vpc_id      = module.vpc.vpc_id

  ingress {
    description = "VPCã‹ã‚‰ã®å…¨ãƒˆãƒ©ãƒ•ã‚£ãƒƒã‚¯"
    from_port   = 0
    to_port     = 65535
    protocol    = "tcp"
    cidr_blocks = [module.vpc.vpc_cidr_block]
  }

  egress {
    description = "å…¨ã‚¢ã‚¦ãƒˆãƒã‚¦ãƒ³ãƒ‰ãƒˆãƒ©ãƒ•ã‚£ãƒƒã‚¯"
    from_port   = 0
    to_port     = 0
    protocol    = "-1"
    cidr_blocks = ["0.0.0.0/0"]
  }

  tags = {
    Name = "${var.project_name}-${var.environment}-node-group-sg"
  }
}

# RDS PostgreSQLãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹
resource "aws_db_subnet_group" "main" {
  name       = "${var.project_name}-${var.environment}-db-subnet-group"
  subnet_ids = module.vpc.database_subnets

  tags = {
    Name = "${var.project_name}-${var.environment}-db-subnet-group"
  }
}

resource "aws_security_group" "rds" {
  name_prefix = "${var.project_name}-${var.environment}-rds"
  vpc_id      = module.vpc.vpc_id

  ingress {
    description = "PostgreSQL"
    from_port   = 5432
    to_port     = 5432
    protocol    = "tcp"
    cidr_blocks = [module.vpc.vpc_cidr_block]
  }

  tags = {
    Name = "${var.project_name}-${var.environment}-rds-sg"
  }
}

resource "aws_db_instance" "main" {
  identifier = "${var.project_name}-${var.environment}-postgres"

  allocated_storage       = 100
  max_allocated_storage   = 1000
  storage_type           = "gp3"
  storage_encrypted      = true
  
  engine         = "postgres"
  engine_version = "16.1"
  instance_class = "db.r6g.xlarge"

  db_name  = "iworkz"
  username = "iworkz_admin"
  password = random_password.db_password.result

  vpc_security_group_ids = [aws_security_group.rds.id]
  db_subnet_group_name   = aws_db_subnet_group.main.name

  backup_retention_period = 7
  backup_window          = "03:00-04:00"
  maintenance_window     = "sun:04:00-sun:05:00"

  multi_az               = true
  publicly_accessible    = false
  copy_tags_to_snapshot  = true
  deletion_protection    = true

  performance_insights_enabled = true
  monitoring_interval         = 60
  enabled_cloudwatch_logs_exports = ["postgresql"]

  tags = {
    Name = "${var.project_name}-${var.environment}-postgres"
  }
}

resource "random_password" "db_password" {
  length  = 32
  special = true
}

# ElastiCache Redisã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼
resource "aws_elasticache_subnet_group" "main" {
  name       = "${var.project_name}-${var.environment}-cache-subnet"
  subnet_ids = module.vpc.private_subnets
}

resource "aws_security_group" "elasticache" {
  name_prefix = "${var.project_name}-${var.environment}-elasticache"
  vpc_id      = module.vpc.vpc_id

  ingress {
    description = "Redis"
    from_port   = 6379
    to_port     = 6379
    protocol    = "tcp"
    cidr_blocks = [module.vpc.vpc_cidr_block]
  }

  tags = {
    Name = "${var.project_name}-${var.environment}-elasticache-sg"
  }
}

resource "aws_elasticache_replication_group" "main" {
  replication_group_id       = "${var.project_name}-${var.environment}-redis"
  description                = "${var.project_name} ${var.environment}ã®Redisã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼"

  port                = 6379
  parameter_group_name = "default.redis7"
  node_type           = "cache.r7g.large"
  num_cache_clusters  = 3

  subnet_group_name       = aws_elasticache_subnet_group.main.name
  security_group_ids      = [aws_security_group.elasticache.id]
  
  at_rest_encryption_enabled = true
  transit_encryption_enabled = true
  auth_token                 = random_password.redis_auth_token.result

  automatic_failover_enabled = true
  multi_az_enabled          = true

  log_delivery_configuration {
    destination      = aws_cloudwatch_log_group.elasticache.name
    destination_type = "cloudwatch-logs"
    log_format       = "text"
    log_type         = "slow-log"
  }

  tags = {
    Name = "${var.project_name}-${var.environment}-redis"
  }
}

resource "random_password" "redis_auth_token" {
  length  = 32
  special = false
}

resource "aws_cloudwatch_log_group" "elasticache" {
  name              = "/aws/elasticache/${var.project_name}-${var.environment}"
  retention_in_days = 14
}

# å‡ºåŠ›
output "vpc_id" {
  description = "VPCã®ID"
  value       = module.vpc.vpc_id
}

output "eks_cluster_endpoint" {
  description = "EKSã‚³ãƒ³ãƒˆãƒ­ãƒ¼ãƒ«ãƒ—ãƒ¬ãƒ¼ãƒ³ã®ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ"
  value       = module.eks.cluster_endpoint
}

output "eks_cluster_name" {
  description = "Kubernetesã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼å"
  value       = module.eks.cluster_name
}

output "rds_endpoint" {
  description = "RDSã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ"
  value       = aws_db_instance.main.endpoint
  sensitive   = true
}

output "redis_endpoint" {
  description = "ElastiCache Redisã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ"
  value       = aws_elasticache_replication_group.main.primary_endpoint_address
  sensitive   = true
}
```

## 12.5 ç›£è¦– & å¯è¦³æ¸¬æ€§

### åŒ…æ‹¬çš„ç›£è¦–ã‚¹ã‚¿ãƒƒã‚¯

**Prometheusã€Grafanaã€Jaegerã®è¨­å®š**
```yaml
# monitoring/prometheus-config.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: prometheus-config
  namespace: monitoring
data:
  prometheus.yml: |
    global:
      scrape_interval: 15s
      evaluation_interval: 15s
      external_labels:
        cluster: 'iworkz-production'
        region: 'ap-northeast-1'

    rule_files:
      - "/etc/prometheus/rules/*.yml"

    alerting:
      alertmanagers:
        - static_configs:
            - targets:
              - alertmanager:9093

    scrape_configs:
      # Kubernetes APIã‚µãƒ¼ãƒãƒ¼
      - job_name: 'kubernetes-apiservers'
        kubernetes_sd_configs:
        - role: endpoints
        scheme: https
        tls_config:
          ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
        bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token
        relabel_configs:
        - source_labels: [__meta_kubernetes_namespace, __meta_kubernetes_service_name, __meta_kubernetes_endpoint_port_name]
          action: keep
          regex: default;kubernetes;https

      # Kubernetesãƒãƒ¼ãƒ‰
      - job_name: 'kubernetes-nodes'
        kubernetes_sd_configs:
        - role: node
        scheme: https
        tls_config:
          ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
        bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token
        relabel_configs:
        - action: labelmap
          regex: __meta_kubernetes_node_label_(.+)

      # Kubernetesãƒãƒƒãƒ‰
      - job_name: 'kubernetes-pods'
        kubernetes_sd_configs:
        - role: pod
        relabel_configs:
        - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_scrape]
          action: keep
          regex: true
        - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_path]
          action: replace
          target_label: __metrics_path__
          regex: (.+)
        - source_labels: [__address__, __meta_kubernetes_pod_annotation_prometheus_io_port]
          action: replace
          regex: ([^:]+)(?::\d+)?;(\d+)
          replacement: $1:$2
          target_label: __address__
        - action: labelmap
          regex: __meta_kubernetes_pod_label_(.+)
        - source_labels: [__meta_kubernetes_namespace]
          action: replace
          target_label: kubernetes_namespace
        - source_labels: [__meta_kubernetes_pod_name]
          action: replace
          target_label: kubernetes_pod_name

      # ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã‚µãƒ¼ãƒ“ã‚¹
      - job_name: 'backend-api'
        static_configs:
        - targets: ['backend-api-service:9090']
        metrics_path: /metrics
        scrape_interval: 10s

      - job_name: 'ai-services'
        static_configs:
        - targets: ['ai-services-service:9090']
        metrics_path: /metrics
        scrape_interval: 15s

      - job_name: 'web-frontend'
        static_configs:
        - targets: ['web-frontend-service:9090']
        metrics_path: /metrics
        scrape_interval: 30s

      # å¤–éƒ¨ã‚µãƒ¼ãƒ“ã‚¹
      - job_name: 'postgres-exporter'
        static_configs:
        - targets: ['postgres-exporter:9187']

      - job_name: 'redis-exporter'
        static_configs:
        - targets: ['redis-exporter:9121']

---
# ã‚¢ãƒ©ãƒ¼ãƒˆãƒ«ãƒ¼ãƒ«è¨­å®š
apiVersion: v1
kind: ConfigMap
metadata:
  name: prometheus-alert-rules
  namespace: monitoring
data:
  alerts.yml: |
    groups:
    - name: iworkz.rules
      rules:
      
      # é«˜ãƒ¬ãƒ™ãƒ«ã‚µãƒ¼ãƒ“ã‚¹å¯ç”¨æ€§
      - alert: ServiceDown
        expr: up == 0
        for: 1m
        labels:
          severity: critical
          team: platform
        annotations:
          summary: "ã‚µãƒ¼ãƒ“ã‚¹ {{ $labels.instance }} ãŒãƒ€ã‚¦ãƒ³ã—ã¦ã„ã¾ã™"
          description: "{{ $labels.job }} ã® {{ $labels.instance }} ãŒ1åˆ†ä»¥ä¸Šãƒ€ã‚¦ãƒ³ã—ã¦ã„ã¾ã™ã€‚"

      # APIãƒ¬ã‚¹ãƒãƒ³ã‚¹æ™‚é–“
      - alert: HighAPILatency
        expr: histogram_quantile(0.95, rate(http_request_duration_seconds_bucket[5m])) > 0.5
        for: 5m
        labels:
          severity: warning
          team: backend
        annotations:
          summary: "é«˜ã„APIãƒ¬ã‚¤ãƒ†ãƒ³ã‚·ãŒæ¤œå‡ºã•ã‚Œã¾ã—ãŸ"
          description: "{{ $labels.instance }} ã®95ãƒ‘ãƒ¼ã‚»ãƒ³ã‚¿ã‚¤ãƒ«ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·ãŒ {{ $value }}s ã§ã™"

      # ã‚¨ãƒ©ãƒ¼ç‡
      - alert: HighErrorRate
        expr: rate(http_requests_total{status=~"5.."}[5m]) / rate(http_requests_total[5m]) > 0.05
        for: 5m
        labels:
          severity: critical
          team: backend
        annotations:
          summary: "é«˜ã„ã‚¨ãƒ©ãƒ¼ç‡ãŒæ¤œå‡ºã•ã‚Œã¾ã—ãŸ"
          description: "{{ $labels.instance }} ã®ã‚¨ãƒ©ãƒ¼ç‡ãŒ {{ $value | humanizePercentage }} ã§ã™"

      # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶š
      - alert: DatabaseConnectionsHigh
        expr: pg_stat_database_numbackends / pg_settings_max_connections * 100 > 80
        for: 5m
        labels:
          severity: warning
          team: database
        annotations:
          summary: "ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶šä½¿ç”¨ç‡ãŒé«˜ã„ã§ã™"
          description: "{{ $labels.instance }} ã§ã®ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶šä½¿ç”¨ç‡ãŒ {{ $value }}% ã§ã™"

      # ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡
      - alert: HighMemoryUsage
        expr: (1 - (node_memory_MemAvailable_bytes / node_memory_MemTotal_bytes)) * 100 > 85
        for: 10m
        labels:
          severity: warning
          team: infrastructure
        annotations:
          summary: "é«˜ã„ãƒ¡ãƒ¢ãƒªä½¿ç”¨ç‡"
          description: "{{ $labels.instance }} ã§ã®ãƒ¡ãƒ¢ãƒªä½¿ç”¨ç‡ãŒ {{ $value }}% ã§ã™"

      # ãƒ‡ã‚£ã‚¹ã‚¯ä½¿ç”¨é‡
      - alert: HighDiskUsage
        expr: (1 - (node_filesystem_avail_bytes / node_filesystem_size_bytes)) * 100 > 85
        for: 5m
        labels:
          severity: warning
          team: infrastructure
        annotations:
          summary: "é«˜ã„ãƒ‡ã‚£ã‚¹ã‚¯ä½¿ç”¨ç‡"
          description: "{{ $labels.instance }} ã§ã®ãƒ‡ã‚£ã‚¹ã‚¯ä½¿ç”¨ç‡ãŒ {{ $value }}% ã§ã™"

      # CPUä½¿ç”¨é‡
      - alert: HighCPUUsage
        expr: 100 - (avg by(instance) (irate(node_cpu_seconds_total{mode="idle"}[5m])) * 100) > 85
        for: 10m
        labels:
          severity: warning
          team: infrastructure
        annotations:
          summary: "é«˜ã„CPUä½¿ç”¨ç‡"
          description: "{{ $labels.instance }} ã§ã®CPUä½¿ç”¨ç‡ãŒ {{ $value }}% ã§ã™"

      # ãƒãƒƒãƒ‰ã‚¯ãƒ©ãƒƒã‚·ãƒ¥ãƒ«ãƒ¼ãƒ—
      - alert: PodCrashLooping
        expr: rate(kube_pod_container_status_restarts_total[15m]) > 0
        for: 5m
        labels:
          severity: critical
          team: platform
        annotations:
          summary: "ãƒãƒƒãƒ‰ãŒã‚¯ãƒ©ãƒƒã‚·ãƒ¥ãƒ«ãƒ¼ãƒ—ã—ã¦ã„ã¾ã™"
          description: "ãƒãƒƒãƒ‰ {{ $labels.namespace }}/{{ $labels.pod }} ãŒæ¯ç§’ {{ $value }} å›å†èµ·å‹•ã—ã¦ã„ã¾ã™"

      # AIã‚µãƒ¼ãƒ“ã‚¹å›ºæœ‰ã®ã‚¢ãƒ©ãƒ¼ãƒˆ
      - alert: AIModelInferenceLatency
        expr: histogram_quantile(0.95, rate(ai_model_inference_duration_seconds_bucket[5m])) > 2.0
        for: 3m
        labels:
          severity: warning
          team: ai
        annotations:
          summary: "AIãƒ¢ãƒ‡ãƒ«æ¨è«–ã®ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·ãŒé«˜ã„ã§ã™"
          description: "ãƒ¢ãƒ‡ãƒ« {{ $labels.model_name }} ã®95ãƒ‘ãƒ¼ã‚»ãƒ³ã‚¿ã‚¤ãƒ«æ¨è«–ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·ãŒ {{ $value }}s ã§ã™"

      - alert: AIModelAccuracyDrop
        expr: ai_model_accuracy < 0.85
        for: 10m
        labels:
          severity: critical
          team: ai
        annotations:
          summary: "AIãƒ¢ãƒ‡ãƒ«ã®ç²¾åº¦ãŒä½ä¸‹ã—ã¾ã—ãŸ"
          description: "ãƒ¢ãƒ‡ãƒ« {{ $labels.model_name }} ã®ç²¾åº¦ãŒ {{ $value }} ã§ã€é–¾å€¤0.85ã‚’ä¸‹å›ã£ã¦ã„ã¾ã™"

---
# Grafanaãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ ConfigMap
apiVersion: v1
kind: ConfigMap
metadata:
  name: grafana-dashboards
  namespace: monitoring
data:
  iworkz-overview.json: |
    {
      "dashboard": {
        "id": null,
        "title": "iWORKZ ãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ æ¦‚è¦",
        "tags": ["iworkz", "overview"],
        "timezone": "Asia/Tokyo",
        "refresh": "30s",
        "time": {
          "from": "now-1h",
          "to": "now"
        },
        "panels": [
          {
            "title": "ã‚µãƒ¼ãƒ“ã‚¹æ­£å¸¸æ€§",
            "type": "stat",
            "targets": [
              {
                "expr": "up",
                "legendFormat": "{{ instance }}"
              }
            ],
            "fieldConfig": {
              "defaults": {
                "mappings": [
                  {
                    "options": {
                      "0": {
                        "text": "DOWN",
                        "color": "red"
                      },
                      "1": {
                        "text": "UP",
                        "color": "green"
                      }
                    },
                    "type": "value"
                  }
                ]
              }
            }
          },
          {
            "title": "ãƒªã‚¯ã‚¨ã‚¹ãƒˆç‡",
            "type": "graph",
            "targets": [
              {
                "expr": "sum(rate(http_requests_total[5m])) by (service)",
                "legendFormat": "{{ service }}"
              }
            ],
            "yAxes": [
              {
                "label": "1ç§’ã‚ãŸã‚Šã®ãƒªã‚¯ã‚¨ã‚¹ãƒˆ"
              }
            ]
          },
          {
            "title": "ãƒ¬ã‚¹ãƒãƒ³ã‚¹æ™‚é–“ï¼ˆ95ãƒ‘ãƒ¼ã‚»ãƒ³ã‚¿ã‚¤ãƒ«ï¼‰",
            "type": "graph",
            "targets": [
              {
                "expr": "histogram_quantile(0.95, sum(rate(http_request_duration_seconds_bucket[5m])) by (le, service))",
                "legendFormat": "{{ service }}"
              }
            ],
            "yAxes": [
              {
                "label": "ãƒ¬ã‚¹ãƒãƒ³ã‚¹æ™‚é–“ (s)"
              }
            ]
          },
          {
            "title": "ã‚¨ãƒ©ãƒ¼ç‡",
            "type": "graph",
            "targets": [
              {
                "expr": "sum(rate(http_requests_total{status=~\"5..\"}[5m])) by (service) / sum(rate(http_requests_total[5m])) by (service)",
                "legendFormat": "{{ service }}"
              }
            ],
            "yAxes": [
              {
                "label": "ã‚¨ãƒ©ãƒ¼ç‡ (%)",
                "max": 1,
                "min": 0
              }
            ]
          },
          {
            "title": "ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ãƒ¦ãƒ¼ã‚¶ãƒ¼",
            "type": "stat",
            "targets": [
              {
                "expr": "sum(active_users_total)",
                "legendFormat": "ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ãƒ¦ãƒ¼ã‚¶ãƒ¼"
              }
            ]
          },
          {
            "title": "AIãƒ¢ãƒ‡ãƒ«ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹",
            "type": "graph",
            "targets": [
              {
                "expr": "ai_model_accuracy",
                "legendFormat": "{{ model_name }} ç²¾åº¦"
              },
              {
                "expr": "histogram_quantile(0.95, rate(ai_model_inference_duration_seconds_bucket[5m]))",
                "legendFormat": "{{ model_name }} P95 ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·"
              }
            ]
          },
          {
            "title": "ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹",
            "type": "graph",
            "targets": [
              {
                "expr": "pg_stat_database_tup_returned / pg_stat_database_tup_fetched",
                "legendFormat": "ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ’ãƒƒãƒˆç‡"
              },
              {
                "expr": "pg_stat_database_numbackends",
                "legendFormat": "ã‚¢ã‚¯ãƒ†ã‚£ãƒ–æ¥ç¶š"
              }
            ]
          },
          {
            "title": "ã‚¤ãƒ³ãƒ•ãƒ©ã‚¹ãƒˆãƒ©ã‚¯ãƒãƒ£ãƒªã‚½ãƒ¼ã‚¹",
            "type": "graph",
            "targets": [
              {
                "expr": "100 - (avg(irate(node_cpu_seconds_total{mode=\"idle\"}[5m])) * 100)",
                "legendFormat": "CPUä½¿ç”¨ç‡ %"
              },
              {
                "expr": "(1 - (node_memory_MemAvailable_bytes / node_memory_MemTotal_bytes)) * 100",
                "legendFormat": "ãƒ¡ãƒ¢ãƒªä½¿ç”¨ç‡ %"
              },
              {
                "expr": "(1 - (node_filesystem_avail_bytes / node_filesystem_size_bytes)) * 100",
                "legendFormat": "ãƒ‡ã‚£ã‚¹ã‚¯ä½¿ç”¨ç‡ %"
              }
            ]
          }
        ]
      }
    }
```

---

**ã“ã®åŒ…æ‹¬çš„ãªãƒ‡ãƒ—ãƒ­ã‚¤ & ã‚¤ãƒ³ãƒ•ãƒ©ã‚¹ãƒˆãƒ©ã‚¯ãƒãƒ£ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã¯ã€ã‚¨ãƒ³ã‚¿ãƒ¼ãƒ—ãƒ©ã‚¤ã‚ºã‚°ãƒ¬ãƒ¼ãƒ‰ã®ã‚¤ãƒ³ãƒ•ãƒ©ã‚¹ãƒˆãƒ©ã‚¯ãƒãƒ£ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã€ã‚³ãƒ³ãƒ†ãƒŠåŒ–æˆ¦ç•¥ã€CI/CDãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã€Infrastructure as Codeã€æ—¥æœ¬å¸‚å ´å‘ã‘ã«æœ€é©åŒ–ã•ã‚ŒãŸé«˜å¯ç”¨æ€§ã€ã‚¹ã‚±ãƒ¼ãƒ©ãƒ“ãƒªãƒ†ã‚£ã€ã‚³ãƒ³ãƒ—ãƒ©ã‚¤ã‚¢ãƒ³ã‚¹è¦ä»¶ã‚’æŒã¤ç›£è¦–ã‚·ã‚¹ãƒ†ãƒ ã®è©³ç´°ãªå®Ÿè£…ã‚’æä¾›ã—ã¾ã™ã€‚**

---

*ã‚¤ãƒ³ãƒ•ãƒ©ã‚¹ãƒˆãƒ©ã‚¯ãƒãƒ£å®Ÿè£…ã¯ã€æ—¥æœ¬ã§ã®ãƒŸãƒƒã‚·ãƒ§ãƒ³ã‚¯ãƒªãƒ†ã‚£ã‚«ãƒ«ãªé›‡ç”¨ãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ é‹ç”¨ã«é©ã—ãŸã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ã€ã‚³ãƒ³ãƒ—ãƒ©ã‚¤ã‚¢ãƒ³ã‚¹ã€é‹ç”¨å„ªç§€æ€§ã‚’é‡è¦–ã—ã€åŒ…æ‹¬çš„ãªç›£è¦–ã€è‡ªå‹•åŒ–ã•ã‚ŒãŸãƒ‡ãƒ—ãƒ­ã‚¤ãƒ—ãƒ­ã‚»ã‚¹ã€ç½å®³å¾©æ—§æ©Ÿèƒ½ã‚’å‚™ãˆã¦è¨­è¨ˆã•ã‚Œã¦ã„ã¾ã™ã€‚*